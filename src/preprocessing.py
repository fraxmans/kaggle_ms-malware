import numpy as np
import pandas as pd
from tqdm import tqdm

import gc
import datetime
import os
import copy
import sys
import time
from multiprocessing import Process, Pool, Manager 
from distutils.version import StrictVersion, LooseVersion

def reduce_mem_usage(df):
    """ iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        
    """
    start_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))

    for col in df.columns:
        col_type = df[col].dtype

        if col_type != object:
            c_min = df[col].min()
            c_max = df[col].max()
        
        if str(col_type)[:3] == 'int':
            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                df[col] = df[col].astype(np.int8)
            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                df[col] = df[col].astype(np.int16)
            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                df[col] = df[col].astype(np.int32)
            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                df[col] = df[col].astype(np.int64)  
        else:
            if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                df[col] = df[col].astype(np.float16)
            elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                df[col] = df[col].astype(np.float32)
            else:
                df[col] = df[col].astype(np.float64)

    end_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))

    return df

def version_factorize(data):
    uniques = pd.unique(data).tolist()
    uniques.sort(key=lambda x: LooseVersion(x))

    u_dict = {}
    for idx, uq in enumerate(uniques):
        u_dict[uq] = idx

    labels = data.apply(lambda x: u_dict[x])
    labels = labels.values

    return labels

def frequency_encoding(data):
    uniques = pd.unique(data)
    u_dict = {}

    count = data.value_counts(normalize=True)
    for idx, val in zip(count.index, count):
        u_dict[idx] = val

    labels = data.apply(lambda x: u_dict[x])
    labels = labels.values

    return labels

def factorize(args):
    data = args[0]
    col = args[1]
    queue = args[2]

    sys.stderr.write("[%s] Processing %s\n" % (datetime.datetime.now(), col))
    if col in ["EngineVersion", "AppVersion", "AvSigVersion", "Census_OSVersion"]:
        labels = frequency_encoding(data)
    else:
        labels, uniques = pd.factorize(data)
    
    queue.put([labels, col])

def merge(args):
    queue = args[0]
    data_dict = {}

    while(1):
        q_args = None
        try:
            q_args = queue.get(timeout=60)
        except:
            df = pd.DataFrame(data_dict)
            queue.put([df])
            return

        labels = q_args[0]
        col = q_args[1]
        data_dict[col] = labels

        sys.stderr.write("[%s] Merge: %s\n" % (datetime.datetime.now(), col))
        
        del labels, col

def col_factorize(data):
    data_obj = data.select_dtypes(include="object")
    sub_args = []
    queue = Manager().Queue()

    for col in data_obj.columns:
        sub_args.append([data[col], col, queue])

    task = []
    for sub_arg in sub_args:
        task.append(Process(target=factorize, args=[sub_arg]))

    for p in task:
        p.start()
    
    for p in task:
        p.join()

    q_args = queue.get(False)
    ret_data_dict = {} 
    while(q_args != None):
        col = q_args[0]
        col_name = q_args[1]

        ret_data_dict[col_name] = col

        try:
            q_args = queue.get(False)
        except:
            break
    
    ret_data = pd.DataFrame(ret_data_dict)

    data_non_obj = data.select_dtypes(exclude="object").copy()
    data_non_obj.index = range(data_non_obj.shape[0])
    ret = pd.concat([data_non_obj, ret_data], axis=1)

    return ret 
